<p>There are the <a href="machine_learning.py">code</a> and its <a href="machine_learning_output.txt">output</a> that cover all the steps described in this section.</p>

<h3 id="decisiontrees">Decision Trees</h3>

<p>I perform decision tree analysis to test nonlinear relationships among a set of explanatory variables (amount of residual sugar and alcohol in wine) and a binary, categorical response variable (<code>0</code> - if the quality of a wine sample is 3, 4, or 5, <code>1</code> - if 6, 7, 8, or 9). All possible cut points (for explanatory variables) are tested. For the present analysis, the entropy “goodness of split” criterion was used to grow the tree and a cost complexity algorithm was used for pruning the full tree into a final subtree. A classification tree was build for each of the wine sets (red and white) separately. In each set, 60% of the samples were used for the training, and 40% - for testing. </p>

<p>The accuracy of the resulted tree for the <strong>red</strong> wine is <code>0.65</code>. The confusion matrix is:</p>

<p><code>[198  105]</code></p>

<p><code>[120  217]</code></p>

<p>The accuracy of the resulted tree for the <strong>white</strong> wine is <code>0.72</code>. The confusion matrix is:</p>

<p><code>[404  262]</code></p>

<p><code>[290  1004]</code></p>

<p>The resulted trees are too big to be examined and visualized. It might indicate that the selected variables are not suitable for proper tree formation, or that the tree analysis is not suitable for these data. The work on this problem is continued in the next paragraph.</p>

<h3 id="randomforests">Random Forests</h3>

<p>Here I perform random forest analysis to evaluate the importance of all the explanatory variables in predicting the quality of wine (binary target variable: <code>0</code> - if the quality of a wine sample is 3, 4, or 5, <code>1</code> - if 6, 7, 8, or 9). Analysis was performed for each wine set (red and white) separately. In each set, 60% of the sample were used for the training, and 40% - for testing. </p>

<p>The analysis consists of two steps. Firstly, I create the random forest model with <code>25</code> trees and examine its results. Secondly, I train random forests with different numbers of trees (1-100) to see the effect of the number on the accuracy of the prediction.</p>

<p>The <a href="machine_learning_output.txt">results</a> of the random forest model with <code>25</code> trees for the <strong>red</strong> wine show that the accuracy of the prediction is <code>0.778</code> and the most important predictor is <code>alcohol</code>, followed by <code>volatile acidity</code>, <code>sulphates</code>, and <code>total sulfur dioxide</code>. It is interesting to note that the results of the <em>multivariate regression</em> for the <strong>red</strong> wine mark different set of variables (<code>chlorides</code>, <code>volatile acidity</code>, <code>sulphates</code>, and <code>pH</code>) as the most influential variables in describing the quality of wine.</p>

<p>The <a href="machine_learning_output.txt">results</a> of the random forest model with <code>25</code> trees for the <strong>white</strong> wine show that the accuracy of the prediction is <code>0.816</code> and the most important predictor is <code>alcohol</code>, followed by <code>volatile acidity</code>, and <code>density</code>. It is interesting to notice that the results of the <em>multivariate regression</em> for the <strong>white</strong> wine mark the same set of variables (<code>alcohol</code>, <code>volatile acidity</code>, and <code>density</code>) as the most influential variables in describing the quality of wine.</p>

<p>Training random forests with different numbers of trees (1-100) shows that, after approximately 20 trees, the subsequent growing of number of trees adds little to the overall accuracy of the forest. It is true for both sets of wine: red and white.</p>
<center>
<p><strong>Red wine:</strong></p>
<img src="red_random_forests.png" alt="" /></p>

<p><strong>White wine:</strong></p>
<img src="white_random_forests.png" alt="" /></p>
</center>

<h3 id="lassoregression">Lasso Regression</h3>

<p>A lasso regression analysis was conducted to identify a subset of wine characteristics (predictor variables) that are best in predicting wine quality (quantitative response variable). All wine characteristics were included as the predictor variables; they are quantitative and were standardized to have a mean of zero and a standard deviation of one.</p>

<p>Data were randomly split into a training set (70% of the observations) and a test set (30%). The least angle regression algorithm with k=10 fold cross validation was used to estimate the lasso regression model in the training set, and the model was validated using the test set. The change in the cross validation mean squared error at each step was used to identify the best subset of predictor variables.</p>
<center>
<p><strong>Red wine:</strong></p>
<img src="red_lasso_coef-vs-alph.png" alt="" />
<img src="red_lasso_mse-vs-alph.png" alt="" />
</center>
<center>
<p><strong>White wine:</strong></p>
<img src="white_lasso_coef-vs-alph.png" alt="" />
<img src="white_lasso_mse-vs-alph.png" alt="" />
</center>

<p>Not all predictor variables were retained in the selected models. For the red wine, <code>citric acid</code>, <code>fixed acidity</code>, and <code>free sulfur dioxide</code> variables received zero coefficients, and therefore do not participate in the prediction. The results of the training indicate the <code>alcohol</code>, <code>volatile acidity</code>, and <code>sulphates</code> variables as the most strongly associated with the quality of wine and, therefore, the most influential for the prediction. Interestingly, this results differ from the ones of random forests analysis and multivariate regression . Mean squared error and R-squared values prove the model being robust for testing on new examples. The predictors account for 33% of the variance in the target variable.</p>

<p>For the white wine, <code>citric acid</code> and <code>fixed acidity</code> variables received zero coefficients, and therefore do not participate in the prediction. The results of the training indicate the <code>alcohol</code>, <code>residual sugar</code>, <code>density</code>, and <code>volatile acidity</code> variables as the most strongly associated with the quality of wine and, therefore, the most influential for the prediction. Interestingly, this results similar to the ones of random forests analysis and multivariate regression. Mean squared error and R-squared values prove the model being robust for testing on new examples. The predictors account for 28% of the variance in the target variable.</p>

<p>We can see that for the white wine the predictive algorithms are more unanimous in the selection of most influential predictors than they are for red.</p>

<h3 id="kmeansclusteranalysis">K-Means Cluster Analysis</h3>

<p>A K-means cluster analysis was conducted to identify underlying subgroups of wine samples based on their characteristics (quantitative clustering variables): density, alcohol, sulphates, pH, volatile acidity, chlorides, fixed acidity,
citric acid, residual sugar, free sulfur dioxide, and total sulfur dioxide (i.e., all available characteristics, expect the wine quality). Analysis was performed for each wine set (red and white) separately. All the variables were standardized to have a mean of 0 and a standard deviation of 1.</p>

<p>Data were randomly split into a training set (70%) and a test set (30% of the observations). A series of k-means cluster analyses were conducted on the training set specifying k=1-9 clusters, using Euclidean distance. The average distance from observations to the cluster centroids was plotted for each of the nine cluster solutions in an elbow curve to provide guidance for choosing the number of clusters to interpret.</p>
<center>
<p><strong>Red wine:</strong></p>
<img src="red_kmeans_elbow.png" alt="" />

<p><strong>White wine:</strong></p>
<img src="white_kmeans_elbow.png" alt="" />
</center>

<p>The elbow curves for both wine sets were inconclusive, suggesting that the 2, 3, 5, and 7-cluster solutions might be interpreted. To choose the best solution, canonical discriminant analyses were further performed for each solution.</p>

<p>Canonical discriminant analysis reduces the 11 clustering variables down to 2 canonical variables that account for most of the variance in the clustering variables. After plotting the canonical variables for each 1-9 cluster solution, the 2-cluster solution was chosen as the one that splits the data in the best way. For the white wine, the plot for 2-cluster solution indicates that the observations in the clusters are densely packed with relatively low within cluster variances, and the clusters don't overlap with each other. For the red wine, the observations in each of the 2 clusters have greater spread suggesting higher within cluster variance, but the clusters also don't overlap with each other.</p>
<center>
<p><strong>Red wine:</strong></p>
<img src="red_2clusters.png" alt="" />

<p><strong>White wine:</strong></p>
<img src="white_2clusters.png" alt="" />
</center>

<p>For the red wine, the means of the clustering variables show that, the samples in the first cluster, on average, have higher values of pH, volatile acidity, free sulfur dioxide, and total sulfur dioxide than the samples in the second cluster. Mean values of all other clustering variables in the first cluster are lower than in the second. </p>

<p>For the white wine, the means of the clustering variables show that, the samples in the first cluster, on average, have higher values of alcohol and pH than the samples in the second cluster. Mean values of all other clustering variables in the first cluster are lower than in the second. </p>

<p>In order to externally validate the clusters, an Analysis of Variance (ANOVA) was conducted to test for significant differences between the clusters on the quality of wine:</p>

<ul>
<li>For the red wine, results indicated significant differences between the clusters on the wine quality (<code>F(1, 1117) = 61.10, p = 1.25e-14</code>). The Tukey post hoc comparisons also showed significant differences between two clusters on the wine quality. Samples in the first cluster have lower quality (<code>mean = 5.502833, std = 0.746143</code>) than the samples in the second cluster (<code>mean = 5.886199, std = 0.864133</code>).</li>

<li>For the white wine, results also indicated significant differences between the clusters on the wine quality (<code>F(1, 3426) = 219.8, p = 3.08e-48</code>). The Tukey post hoc comparisons also showed significant differences between two clusters on the wine quality. Samples in the first cluster have higher quality (<code>mean = 6.057617, std = 0.924930</code>) than the samples in the second cluster (<code>mean = 5.610145, std = 0.772186</code>).</li>
</ul>
